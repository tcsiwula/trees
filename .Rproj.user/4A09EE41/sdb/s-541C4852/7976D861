{
    "collab_server" : "",
    "contents" : "Decision Trees\n========================================================\n\nWe will have a look at the `Carseats` data using the `tree` package in R, as in the lab in the book.\nWe create a binary response variable `High` (for high sales), and we include it in the same dataframe.\n```{r}\nrequire(ISLR)\n#install.packages('tree', repos=\"http://cran.r-project.org\")\nrequire(tree)\nattach(Carseats)\nhist(Sales)\nHigh=ifelse(Sales<=8,\"No\",\"Yes\")\nCarseats=data.frame(Carseats, High)\n```\nRandom Forests and Boosting\n============================\n\nThese methods use trees as building blocks to build more complex models. Here we will use the Boston housing data to explore random forests and boosting. These data are in the `MASS` package.\nIt gives housing values and other statistics in each of 506 suburbs of Boston based on a 1970 census.\n\nRandom Forests\n--------------\nRandom forests build lots of bushy trees, and then average them to reduce the variance.\n\n```{r}\ninstall.packages('randomForest', repos=\"http://cran.r-project.org\")\nrequire(randomForest)\nrequire(MASS)\nset.seed(101)\ndim(Boston)\ntrain=sample(1:nrow(Boston),300)\n?Boston\n```\nLets fit a random forest and see how well it performs. We will use the response `medv`, the median housing value (in \\$1K dollars)\n\n```{r}\nrf.boston=randomForest(medv~.,data=Boston,subset=train)\nrf.boston\n```\nThe MSR and % variance explained are based on OOB  or _out-of-bag_ estimates, a very clever device in random forests to get honest error estimates. The model reports that `mtry=4`, which is the number of variables randomly chosen at each split. Since $p=13$ here, we could try all 13 possible values of `mtry`. We will do so, record the results, and make a plot.\n\n```{r}\noob.err=double(13)\ntest.err=double(13)\nfor(mtry in 1:13){\n  fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)\n  oob.err[mtry]=fit$mse[400]\n  pred=predict(fit,Boston[-train,])\n  test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))\n  cat(mtry,\" \")\n}\nmatplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c(\"red\",\"blue\"),type=\"b\",ylab=\"Mean Squared Error\")\nlegend(\"topright\",legend=c(\"OOB\",\"Test\"),pch=19,col=c(\"red\",\"blue\"))\n```\n\nNot too difficult! Although the test-error curve drops below the OOB curve, these are estimates based on data, and so have their own standard errors (which are typically quite large). Notice that the points at the end with `mtry=13` correspond to bagging.\n\n \n",
    "created" : 1479075523048.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1227408437",
    "id" : "7976D861",
    "lastKnownWriteTime" : 1477927894,
    "last_content_update" : 1479215777668,
    "path" : "~/Dropbox/cs451_fall_2016/trees/trees-ensemble-methods.Rmd",
    "project_path" : "trees-ensemble-methods.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}